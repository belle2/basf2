dataset:
  # path to the root folder containing train/val subdirectories for training
  path:  # /home/my_user_name/some_folder/maybe_another_one/root
  config:
    # Node input features of the model
    features: ['electronID_noSVD_noTOP', 'protonID_noSVD', 'muonID_noSVD', 'kaonID_noSVD', 'pionID_noSVD', 'pt', 'pz', 'dr', 'dz', 'clusterNHits', 'clusterTiming', 'clusterE9E25', 'charge'] #'PDG', 'clusterReg']  # Input features to use (overrides "ignore" param)
    # Edge input features
    edge_features: [costheta, doca]
    # Global input features (null to not use global features)
    global_features: null
    # You can configure simple normalization of features
    # They will be executed in the order given
    # Supported processes:
    # [power, <pow>] = x^pow
    # [linear, <mu>, <std>] = (x - mu)/std
    normalize:
      clusterNHits: [[linear, 6.9, 3.2]]
      clusterTiming: [[power, 0.5], [linear, -0.5, 4.1]]
      clusterE9E25: [[linear, 0.96, 0.06]]
      pt: [[power, 0.5], [linear, 0.5, 0.2]]
      pz: [[power, 0.5], [linear, 0.42, 0.23]]
      dr: [[power, 0.25], [linear, 0.16, 0.19]]
      dz: [[power, 0.25], [linear, 0.41, 0.15]]
      doca: [[linear, 0.02, 0.04]]
output:
  # Top directory to save model (null to not save it, e.g. for testing). run_name as subdir and timestamp suffix will be added automatically
  path: null # /home/my_name/some_output_folder 
  # Name of the training
  run_name: test
train:
  # Number of training epochs
  epochs: 3
  # Number of training samples in each batch
  batch_size: 128
  # Number of epochs before training is pruned if no improvement is found 
  early_stop_patience: 100
  # Learning rate of the model
  learning_rate: 0.001
  # Whether to use a learning rate scheduler
  lr_scheduler: False
  # Whether to use mixed precision for faster training
  mixed_precision: True
  # Compile the model to have a speed-up (requires PyTorch >= 2.0)
  compile_model: False
  # Random seed
  seed: 42
model:
  # Whether to enable global layer in the model
  global_layer: True
  # compute 'mean' or 'sum' of losses across the batch. 'sum' is unstable and leads to errors during training
  loss_reduction: mean
  # Parameter to tune the learning of masses of FSP
  alpha_mass: 1 
  # Dimension of hidden layers
  hidden_layer_dim: 512
  # Number of hidden layers
  num_hid_layers: 1
  # Number of intermediate MetaLayers
  num_ML: 1
  # Dropout rate
  dropout: 0.3