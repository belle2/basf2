dataset:
  # Select the data type, this decides which Dataset class to use
  inMemory: True
  path: /home/jcerasol/Code/local_basf2/graFEI_development/analysis/scripts/grafei/scripts/ntuples_test_B # Should contain train/val subdirectories
  config:
    features: ['electronID_noSVD_noTOP', 'protonID_noSVD', 'muonID_noSVD', 'kaonID_noSVD', 'pionID_noSVD', 'pt', 'pz', 'dr', 'dz', 'clusterNHits', 'clusterTiming', 'clusterE9E25', 'charge'] #'PDG', 'clusterReg']  # Input features to use (overrides "ignore" param)
    edge_features: [costheta, doca]
    global_features: null #[nParticles]
    #ignore: ['isSignal']  # Input features to ignore
    subset_unmatched: True
    # allow_background: False
    monitor_tag: 'Validation'  # Which dataset tag to monitor for e.g. checkpointing
    # You can configure simple normalization of features
    # They will be executed in the order given
    # Supported processes:
    # [power, <pow>] = x^pow
    # [linear, <mu>, <std>] = (x - mu)/std
    normalize:
      clusterNHits: [[linear, 6.9, 3.2]]
      clusterTiming: [[power, 0.5], [linear, -0.5, 4.1]]
      clusterE9E25: [[linear, 0.96, 0.06]]
      # p: [[power, 0.5], [linear, 0.6, 0.2]]
      pt: [[power, 0.5], [linear, 0.5, 0.2]]
      # E: [[power, 0.5], [linear, 0.6, 0.25]]
      pz: [[power, 0.5], [linear, 0.42, 0.23]]
      dr: [[power, 0.25], [linear, 0.16, 0.19]]
      dz: [[power, 0.25], [linear, 0.41, 0.15]]
      doca: [[linear, 0.02, 0.04]]
      # nParticles: [[linear, 11.4, 3.6]]
      #electronID : [[linear, 0.1, 0.25]]
      #kaonID : [[linear, 0.14, 0.33]]
      #muonID : [[linear, 0.19, 0.3]]
      #protonID : [[linear, 0.03, 0.14]]
      #pionID : [[linear, 0.53, 0.41]]
  edge_classes: 6 # 6 (7): 0, 1, 2, 3, 4, 5(, 6)
output:
  # Top directory to save model, run_name as subdir and timestamp suffix
  # will be added automatically
  path: #/home/jcerasol/Code/local_basf2/graFEI_development/analysis/scripts/grafei/scripts #null 
  tensorboard: null #/home/jcerasol/Code/local_basf2/graFEI_development/analysis/scripts/grafei/scripts #null
  # Give a name to this config file's series of training
  # Tensorboard logs will be saved in a subdir with that name to help 
  # organise them
  run_name: test_B
train:
  epochs: 3
  batch_size: 128
  num_workers: 0
  # Whether to calculate class weights from dataset
  early_stop_patience: 100
  learning_rate: 0.001
  lr_scheduler: False
  l2_loss: 0 # L2 regularization term
  mixed_precision: True
  class_weights: False
  progress_bar: False
  record_gpu_usage: False
  include_efficiency: False
  compile_model: False
  seed: 42 #seed
  optuna:
    explore:
      # Optuna hyperparameters to vary
      # NOTE: first is type (categorical, int, float, discrete_uniform, loguniform, uniform), then the needed arguments.
      #       See https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#
      learning_rate: [loguniform, 1.e-5, 1]
      batch_size: [categorical, [32, 64, 128, 256, 512, 1024]]
      l2_loss: [loguniform, 1.e-7, 0.1]
    ntrials: 1000
    timeout: 604800 # in seconds
    # One study for many trials
    study_path: ./results/optuna/
val:
  num_workers: 0
geometric_model:
  overwrite: True # Process the graphs from scratch
  global_layer: False
  loss_reduction: mean #mean #sum
  # alpha_prob: 0 # 0.1 # Parameter to tune the learning of B probability
  # alpha_momentum: 0 # 0.1 # Parameter to tune the learning of B momentum
  alpha_mass: 1 # 0.1 # Parameter to tune the learning of masses of FSP
  hidden_layer_dim: 512
  num_hid_layers: 1
  num_ML: 1
  dropout: 0.3
  symmetrize: True
  normalize: batchnorm #batchnorm layernorm null
  # Optuna hyperparameters to vary
  # NOTE: first is type (categorical, int, float, discrete_uniform, loguniform, uniform), then the needed arguments.
  #       See https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#
  optuna:
    hidden_layer_dim: [categorical, [64, 128, 256, 512, 1024]]
    num_hid_layers: [int, 1, 3]
    num_ML: [int, 1, 3]
    dropout: [uniform, 0, 1]
    # normalize: [categorical, [batchnorm, layernorm]] # categorical parameters need a list as input
