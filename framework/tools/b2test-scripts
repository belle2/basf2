#!/usr/bin/env python3

##########################################################################
# basf2 (Belle II Analysis Software Framework)                           #
# Author: The Belle II Collaboration                                     #
#                                                                        #
# See git log for contributors and copyright holders.                    #
# This file is licensed under LGPL-3.0, see LICENSE.md.                  #
##########################################################################

"""\
Run basf2 framework tests of the given packages.  If no argument is given all
tests are run.

For each package execute all files ending in ``*.py`` inside the ``tests/``
directory as test cases. A return value of 0 is treaded as success, other return
values are treated as failures. In addition if ``*.out`` or ``*.err`` files with
the same base name are present in the tests directory they will be treated as
expected output on stdout or stderr respectively. If the output of the script
differes from the contents of these files the test is marked as failed as well.
The first line from stdout will not be compared to allow executing the scripts
from a different directory.

.. note::

   Tests can decide that they need to be skipped under certain conditions. To do so
   a test script can write a line beginning with ``TEST SKIPPED:`` followed by the
   reason for skipping on stderr and exit with a non-zero return code.

.. note::

   On Unix/Linux systems, exit codes are defined to be 8-bit unsigned value.
   Values outside the 0-255 range are truncated by taking modulo 256. If the
   passed code happens to be a multiple of 256, the test will be a wrongly
   tagged as a ``SUCCESS``. Care should be taken when directly passing return
   codes from other processes.

.. seealso:: `b2test_utils.skip_test`

--epilog--
.. rubric:: Examples

* run all tests with 8 tests being run in parallel::

    $ %(prog)s -j8

* run the framework tests and save the output as XML::

    $ %(prog)s --xml=framework-tests.xml framework

* run only tests which where the python filename contains seqroot::

    $ %(prog)s --filter=seqroot
"""

import sys
import os
import subprocess
import difflib
from concurrent.futures import ThreadPoolExecutor
import threading
import nbformat
from nbconvert.preprocessors import ExecutePreprocessor
from nbconvert.preprocessors.execute import CellExecutionError
import argparse
import re
import datetime
import tempfile
import timeit
import xml.etree.ElementTree as ET
from enum import Enum
from typing import Dict, List, Tuple, Optional

from b2test_utils import temporary_set_environment
# allow stopping tests with Ctrl+c
import signal
signal.signal(signal.SIGINT, signal.SIG_DFL)

# avoid mixing print statements from different threads
print_lock = threading.Lock()


def get_argument_parser():
    """Return an argument parser correctly setup for all known options"""
    description, epilog = __doc__.split("--epilog--", 1)
    parser = argparse.ArgumentParser(description=description, epilog=epilog,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("-j", type=int, default=1, dest="nprocess", metavar="N",
                        help="Number of test cases to run in parallel")
    parser.add_argument("--xml", type=str, default=None, help="Save results into given "
                        "file using xml format compatible with JUnit parsers")
    parser.add_argument("--xml-input", type=str, default="", help="Read previous results from "
                        "file using xml format compatible with JUnit parsers")
    parser.add_argument("-n", "--diff-lines", type=int, default=10,
                        help="Number of lines to show if output is different from template")
    parser.add_argument("-q", "--quiet", action="store_true", default=False,
                        help="if given don't print output of failed tests")
    parser.add_argument("--filter", type=str, default="", help="Only run tests where the name "
                        "of the steering file matches the given regular expression")
    parser.add_argument("--exclude", type=str, default="", help="Exclude all tests where the name "
                        "of the steering file matches the given regular expression")
    parser.add_argument("directories", nargs="*", help="run all tests included in this directory."
                        "If none are specified run all tests", metavar="DIRECTORY")
    parser.add_argument("-t", "--timeout", type=int, default=None, metavar="SECONDS",
                        help="if given this is the maximum runtime in seconds for each "
                        "test case before they are killed and marked as failed")
    return parser


def compare_log(release, name, log, logtype, ignore_lines=0, diff_lines=10):
    """Compare the log output to the content of a template file

    Parameters:
      release: release directory in which test is found
      name: name of the test
      log: output of the test
      logtype: which type of output? should be "out" or "err"
      ignore_lines: how many lines to ignore at the beginning when
        comparing to the template.  Used to strip of steering file name
      diff_lines: how many lines of the diff to print on the console
    """

    template = name[:-2] + logtype
    template_file = os.path.join(release, template)
    if os.path.isfile(template_file):
        diff = list(difflib.diff_bytes(
            difflib.unified_diff,
            open(template_file, "rb").read().splitlines()[ignore_lines:],
            log.splitlines()[ignore_lines:],
            fromfile=template.encode(),
            tofile=(f"std{logtype} of 'basf2 {name}'").encode(),
            lineterm=b'',
            n=0,
            ))
        if len(diff) > 0 and diff_lines > 0:
            # print directly to the raw buffer object as bytes, don't
            # decode/encode first
            sys.stdout.buffer.write(b"\n".join(diff[2:2 + diff_lines]))
            sys.stdout.buffer.write(b"\n")
            sys.stdout.buffer.flush()

        # return the full diff to be included in the xml report as string
        return "\n".join(e.decode(errors="replace") for e in diff)

    return None


def format_output(name, out, err):
    """Nicely format the stdout and stderr of testcase called name"""
    if options.quiet:
        return ""
    output = []
    if out and out.strip():
        output += [f"=== stdout of {name} ===\n", out.decode(errors="replace")]
    if err and err.strip():
        output += [f"=== stderr of {name} ===\n", err.decode(errors="replace")]
    if len(output):
        output.append(f"=== end {name} ===\n")
    return "".join(output)


def run_test(testinformation):
    """ run either regurlar python tests (with basf2) or as jupyter notebooks """

    if testinformation.extension == ".ipynb":
        return run_test_ipynb(testinformation)
    elif testinformation.extension == ".py":
        return run_test_py(testinformation)
    else:
        print(f"Testfile {testinformation.name} with extension {testinformation.extension} not supported")


def run_test_ipynb(testinformation):
    """Run the test within a jupyter notebook
    Currently not supported is skipping ipynb's and forwarding output of the notebook
    """

    global options
    global print_lock

    time_start = timeit.default_timer()

    try:
        with open(testinformation.filename) as f:
            nb = nbformat.read(f, as_version=4)
            # wait 30 seconds for each cell executing before timing out
            ep = ExecutePreprocessor(timeout=30, kernel_name='python3')
            ep.preprocess(nb, resources={})

            testinformation.state = TestState.PASSED
            testinformation.message = "finished successfully"

    # execution errors within the notebook will throw this exception
    except CellExecutionError as ce:
        testinformation.state = TestState.FAILED
        testinformation.message = "A CellExecutionError happened. Please re-run the run-tests stage" \
            " if this failure happened in the GitLab pipeline."
        testinformation.output = str(ce)
    except TimeoutError as te:
        testinformation.state = TestState.TIMEOUT
        testinformation.message = "A TimeoutError happened. Please re-run the run-tests stage"\
            " if this failure happened in the GitLab pipeline."
        testinformation.output = str(te)
    except RuntimeError as re:
        testinformation.state = TestState.TIMEOUT
        testinformation.message = "A RuntimeError happened. Please re-run the run-tests stage"\
            " if this failure happened in the GitLab pipeline."
        testinformation.output = str(re)

    testinformation.time = timeit.default_timer() - time_start

    with print_lock:
        testinformation.report_test_result()

    return testinformation


def run_test_py(testinformation):
    """Run the test for the given steering file"""

    global options
    global print_lock

    # output on dumb terminal to avoid ESC[?1034h in output
    # happens when loading readline https://bugzilla.redhat.com/show_bug.cgi?id=304181
    os.putenv('TERM', 'vt100')

    time_start = timeit.default_timer()
    try:
        process = subprocess.run(['basf2', testinformation.filename], stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE, timeout=options.timeout)
    except subprocess.TimeoutExpired as e:
        # apparently test case took to long so format result message. Without
        # timeout result would be None so we can distinguish later.
        testinformation.state = TestState.TIMEOUT
        testinformation.message = f"timeout of {options.timeout} seconds reached"
        testinformation.output = format_output(testinformation.name, e.stdout, e.stderr)

    testinformation.time = timeit.default_timer() - time_start

    with print_lock:
        if testinformation.state == TestState.TIMEOUT:
            # timeout happened, everything is already setup
            pass
        elif process.returncode != 0:
            # check for skipped test
            skipped = re.search(br"^TEST SKIPPED:\s*(.+)$", process.stderr, re.MULTILINE)
            if skipped:
                testinformation.state = TestState.SKIPPED
                testinformation.message = skipped.group(1).decode()
            else:
                testinformation.state = TestState.FAILED
                testinformation.message = f"exited with return code {int(process.returncode)}"
                testinformation.output = format_output(testinformation.name, process.stdout, process.stderr)
        else:
            log_tests = [
                compare_log(
                    testinformation.release_dir,
                    testinformation.name,
                    process.stdout,
                    'out',
                    1,
                    options.diff_lines),
                compare_log(
                    testinformation.release_dir,
                    testinformation.name,
                    process.stderr,
                    'err',
                    0,
                    options.diff_lines)]
            logs_failed = sum(0 if e is None else len(e) for e in log_tests)
            if log_tests[0] is None and log_tests[1] is None:
                testinformation.state = TestState.PASSED
                testinformation.message = "finished successfully"
            elif logs_failed == 0:
                testinformation.state = TestState.PASSED
                testinformation.message = "logs ok"
            else:
                testinformation.state = TestState.FAILED
                testinformation.message = "logs differ from expectation"
                testinformation.output = "\n".join(e for e in log_tests if e is not None)

        testinformation.report_test_result()

    # we didn't want to print the diff in full but we want to return it if it
    # exists so let's replace the extra
    # if output_diff is not None:
    #     extra = output_diff
    return testinformation


def process_dir(dir, release_dir):
    """Search a directory for prepare/test steering files"""

    exclude_str_dirs = ['include', 'src', 'tools', 'scripts', 'data', 'doc', 'examples']
    tests = []
    for entry in os.listdir(dir):
        dir_entry = os.path.join(dir, entry)
        if entry.find('.') > -1 or not os.path.isdir(dir_entry) or entry in exclude_str_dirs:
            continue
        if entry in ("tests", "prepare"):
            # search for files with .py (basf2 steering files) and .ipynb (jupyter notebooks) extensions
            for root, _, files in os.walk(dir_entry):
                if root.startswith(os.path.join(dir_entry, 'prepare')) or root == dir_entry:
                    for file in files:
                        if file.endswith('.py') or file.endswith('.ipynb'):
                            full_path = os.path.join(root, file)
                            name = full_path[len(release_dir):].lstrip('/')
                            tests.append(TestInformation(name, release_dir))
        else:
            tests += process_dir(dir_entry, release_dir)

    return tests


def process_top_dir(release_dir, directories):
    """Loop over folders in the top release directory"""

    dirs = os.listdir(release_dir)
    if len(directories) > 0:
        dirs = directories

    exclude_str_dirs = ['build', 'include', 'lib', 'bin', 'modules', 'data', 'externals']
    tests = []
    for entry in dirs:
        if entry in exclude_str_dirs:
            continue
        dir_entry = os.path.join(release_dir, entry)
        if entry.find('.') > -1 or not os.path.isdir(dir_entry):
            continue
        tests += process_dir(dir_entry, release_dir)

    return tests


class TestState(Enum):
    """Enum to store the state of a test"""
    DEFAULT = 0
    PASSED = 1
    FAILED = 2
    SKIPPED = 3
    TIMEOUT = 4


class TestInformation:
    """Class to store information about a test"""

    def __init__(self, name: str = "", release_dir: str = "") -> None:
        self.name = name
        self.release_dir = release_dir

        self.state: TestState = TestState.DEFAULT
        self.message: str = ""
        self.time: float = 0  # in s
        self.output: str = ""

        self._suite: str = os.path.dirname(self.name)
        if self._suite.endswith("/tests"):
            self._suite = os.path.dirname(self._suite)

    def __str__(self) -> str:
        return self.name

    @property
    def filename(self) -> str:
        return os.path.join(self.release_dir, self.name)

    @property
    def suite(self) -> str:
        return self._suite

    @suite.setter
    def suite(self, value: str) -> None:
        self._suite = value

    @property
    def extension(self) -> str:
        return os.path.splitext(self.name)[1]

    @property
    def is_prepare(self) -> bool:
        return "prepare" in self.suite

    def report_test_result(self) -> None:
        """ print out the result of one test"""
        print(f"{self.output}\nfinished test {self.name}\n     "
              f"-> {self.state.name}: {self.message}\n     -> exec. time: {self.time}s",
              flush=True)

    def create_testcase_xml_element(self, suite: ET.Element) -> ET.Element:
        test_el = ET.SubElement(
            suite,
            "testcase",
            name=os.path.basename(self.name),
            classname=self.suite,
            time=str(self.time)
        )

        if self.state != TestState.PASSED:
            failure_type = "failure" if self.state != TestState.SKIPPED else "skipped"
            failure_message = self.message
            failure = ET.SubElement(test_el, failure_type, message=failure_message,
                                    type=failure_type.upper())

            if self.output:
                failure.text = re.sub(r'\033\[[0-9;]*m', '', self.output)

        return test_el


class TestSuite:
    """Class to aggregate test case results by directory"""

    def __init__(self, name: str = "") -> None:
        self.name = name
        """Init the suite with empty values"""
        #: passed tests in the suite/package
        self.passed_tests: List[TestInformation] = []
        #: failed tests in the suite/package
        self.failed_tests: List[TestInformation] = []
        #: skipped tests in the suite/package
        self.skipped_tests: List[TestInformation] = []
        #: time in seconds needed for test execution
        self.time: float = 0

    @property
    def passed(self) -> int:
        return len(self.passed_tests)

    @property
    def failed(self) -> int:
        return len(self.failed_tests)

    @property
    def skipped(self) -> int:
        return len(self.skipped_tests)

    def get_tests(self) -> List[TestInformation]:
        return self.passed_tests + self.failed_tests + self.skipped_tests

    @property
    def total(self) -> int:
        return len(self.get_tests())

    @property
    def is_prepare(self) -> bool:
        return "prepare" in self.name

    def add(self, testinformation: TestInformation) -> None:
        """Add a new test to the suite. Return True if test was successful or
        skipped, False if it failed"""
        if testinformation.state == TestState.PASSED:
            self.passed_tests.append(testinformation)
        elif testinformation.state == TestState.SKIPPED:
            self.skipped_tests.append(testinformation)
        else:
            self.failed_tests.append(testinformation)

        self.time += testinformation.time

    def create_testsuite_xml_element(self, root: ET.Element) -> ET.Element:
        suite_el = ET.SubElement(
            root,
            "testsuite",
            name=self.name,
            tests=str(self.total),
            failures=str(self.failed),
            skipped=str(self.skipped),
            time=str(self.time)
        )

        for test in self.get_tests():
            test.create_testcase_xml_element(suite_el)

        return suite_el


class TestResult:
    """Class to store the results of a test run"""

    def __init__(self) -> None:
        self.suites: Dict[str, TestSuite] = {}

    @property
    def total(self) -> int:
        return sum(suite.total for suite in self.suites.values())

    @property
    def failed(self) -> int:
        return sum(suite.failed for suite in self.suites.values())

    @property
    def skipped(self) -> int:
        return sum(suite.skipped for suite in self.suites.values())

    @property
    def time(self) -> float:
        return sum(suite.time for suite in self.suites.values())

    def get_skipped(self) -> List[TestInformation]:
        skipped = []
        for suite in self.suites.values():
            skipped += suite.skipped_tests
        return skipped

    def get_failed(self) -> List[TestInformation]:
        failed = []
        for suite in self.suites.values():
            failed += suite.failed_tests
        return failed

    def add(self, testinformation: TestInformation) -> None:
        suite_name = testinformation.suite
        if suite_name not in self.suites:
            self.suites[suite_name] = TestSuite(suite_name)
        self.suites[suite_name].add(testinformation)

    def create_testresult_xml_element(self, root_el: Optional[ET.Element] = None) -> ET.Element:
        if root_el is None:
            root_el = ET.Element("testsuites",
                                 tests=str(self.total),
                                 failures=str(self.failed),
                                 skipped=str(self.skipped),
                                 timestamp=datetime.datetime.now().isoformat(),
                                 time=str(self.time),
                                 name="basf2 Tests")

        for suite in self.suites.values():
            suite.create_testsuite_xml_element(root_el)

        return root_el

    def report_results_xml(self, filename: str = "", root_el: Optional[ET.Element] = None) -> ET.Element:
        root = self.create_testresult_xml_element(root_el)
        if filename:
            tree = ET.ElementTree(root)
            tree.write(filename, encoding="UTF-8", xml_declaration=True)

        return root

    def report_results(self, prepare_step: bool = True) -> None:
        print(flush=True)
        if self.skipped > 0:
            print("\n    ".join([f"Skipped tests: {test.name} ({test.message})" for test in self.get_skipped()]), flush=True)
        if self.failed > 0:
            print("\n    ".join([f"FAILED tests: {test.name} ({test.message})" for test in self.get_failed()]), flush=True)
        # print summary and exit
        print(f'{self.failed} out of {self.total} {"prepare steps" if prepare_step else "tests"} '
              f'failed, {self.skipped} were skipped\n', flush=True)


class TestScheduler:
    """Class to run tests in parallel"""

    def __init__(
                self,
                xml_input_file: str = "",
                xml_output_file: str = "",
                directories: List[str] = [],
                filter_str: str = "",
                exclude_str: str = "",
                nprocess: int = 1
            ) -> None:
        self.xml_input_file = xml_input_file
        self.xml_output_file = xml_output_file
        self.directories = directories
        self.filter_str = filter_str
        self.exclude_str = exclude_str
        self.nprocess = nprocess

        self.new_suites: Dict[str, TestSuite] = {}
        self.previous_suites: Dict[str, TestSuite] = {}
        self.execute_times: Dict[TestInformation, float] = {}

        self.release_dir: str = os.environ.get("BELLE2_RELEASE_DIR") or os.environ.get("BELLE2_LOCAL_DIR")

    def filter_tests(self, tests: List[TestInformation]) -> List[TestInformation]:
        filtered_tests = tests

        if self.filter_str:
            try:
                filter_regex = re.compile(self.filter_str)
            except re.error as e:
                print(f"The --filter argument '{self.filter_str}' is not a valid regular expression: {e}")
            filtered_tests = [test for test in filtered_tests if filter_regex.search(test.name)]

        if self.exclude_str:
            try:
                exclude_regex = re.compile(self.exclude_str)
            except re.error as e:
                print(f"The --exclude argument '{self.exclude_str}' is not a valid regular expression: {e}")
            filtered_tests = [test for test in filtered_tests if not exclude_regex.search(test.name)]

        return filtered_tests

    def read_results_xml(self) -> None:
        if self.xml_input_file:
            tree: ET.ElementTree = ET.parse(self.xml_input_file)
            root: ET.Element = tree.getroot()

            for suite_el in root.findall("testsuite"):
                suite_name: str = suite_el.get("name")
                suite = TestSuite(suite_name)
                for test_el in suite_el.findall("testcase"):
                    test = TestInformation()
                    test.name = test_el.get("name")
                    test.suite = suite_name
                    test.time = float(test_el.get("time"))
                    suite.add(test)
                self.previous_suites[suite_name] = suite

    def get_previous_tests(self) -> List[TestInformation]:
        tests = []
        for suite in self.previous_suites.values():
            tests += suite.get_tests()
        return tests

    def load_execution_times(self) -> None:
        self.read_results_xml()
        previous_tests = self.get_previous_tests()
        for test in previous_tests:
            self.execute_times[(test.name, test.is_prepare)] = test.time

    def add_new_test(self, testinformation: TestInformation) -> None:
        suite_name = testinformation.suite
        if suite_name not in self.new_suites:
            self.new_suites[suite_name] = TestSuite(suite_name)
        self.new_suites[suite_name].add(testinformation)

    def load_new_tests(self) -> None:
        all_tests = []
        all_tests += process_top_dir(self.release_dir, self.directories)
        for test in all_tests:
            self.add_new_test(test)

    def get_new_tests(self) -> List[TestInformation]:
        tests = []
        for suite in self.new_suites.values():
            tests += suite.get_tests()
        return tests

    def get_tests_to_execute(self) -> Tuple[List[TestInformation]]:
        self.load_execution_times()
        self.load_new_tests()

        new_tests = self.get_new_tests()
        for test in new_tests:
            test.time = self.execute_times.get((os.path.basename(test.name), test.is_prepare), 9000)

        prepare_tests = sorted([test for test in new_tests if test.is_prepare], key=lambda test: test.time, reverse=True)
        other_tests = sorted([test for test in new_tests if not test.is_prepare], key=lambda test: test.time, reverse=True)

        return [prepare_tests, other_tests]

    def _run_tests(self, tests: List[TestInformation], prepare_step: bool = False,
                   root_el: Optional[ET.Element] = None) -> ET.Element:
        print(f'Collected {len(tests)} {"prepare steps" if prepare_step else "tests"}...', flush=True)
        if self.nprocess > 1:
            with ThreadPoolExecutor(max_workers=self.nprocess) as pool:
                # run tests in multiple threads but consume the iterator right away to
                # avoid a bug in python 3.5 when zipping over a futures result iterator
                # and then exiting using sys.exit().
                results = list(pool.map(run_test, tests))
        else:
            results = map(run_test, tests)

        test_results = TestResult()
        for result in results:
            test_results.add(result)

        output_el = test_results.report_results_xml(
            filename=self.xml_output_file if not prepare_step else "",
            root_el=root_el
        )

        # print a list of failed tests to find them more easily
        test_results.report_results(prepare_step)

        if test_results.failed > 0:
            sys.exit(test_results.failed)

        return output_el

    def execute_tests(self):
        tests_to_execute = self.get_tests_to_execute()
        prepare_tests = self.filter_tests(tests_to_execute[0])
        all_tests = self.filter_tests(tests_to_execute[1])

        print(f"Running steering file tests in {self.nprocess} processes", flush=True)
        xml_report_el = None
        if prepare_tests:
            xml_report_el = self._run_tests(tests=prepare_tests, prepare_step=True, root_el=xml_report_el)

        if all_tests:
            _ = self._run_tests(tests=all_tests, prepare_step=False, root_el=xml_report_el)


if __name__ == "__main__":

    # check whether a release is set up
    if not ('BELLE2_LOCAL_DIR' in os.environ or 'BELLE2_RELEASE_DIR' in os.environ):
        sys.stderr.write('Error: no release is set up.\n')
        sys.exit(-1)

    options = get_argument_parser().parse_args()

    # create a temporary directory where to store the files created during the prepare step
    with tempfile.TemporaryDirectory() as prepare_path:
        os.chmod(prepare_path, 0o744)  # necessary for the buildbot
        # and temporary set the BELLE2_PREPARE_PATH variable
        with temporary_set_environment(BELLE2_PREPARE_PATH=prepare_path):

            scheduler = TestScheduler(
                xml_input_file=options.xml_input,
                xml_output_file=options.xml,
                directories=options.directories,
                filter_str=options.filter,
                exclude_str=options.exclude,
                nprocess=options.nprocess
            )

            scheduler.execute_tests()
