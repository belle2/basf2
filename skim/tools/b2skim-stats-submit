#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A script to submit small skim test jobs, and save the output in a form to be
read by ``b2skim-stats-print``.
"""

import argparse
from datetime import datetime as dt
from functools import lru_cache
from getpass import getuser
import json
from os import getenv
from pathlib import Path
import re
from shlex import quote
from socket import gethostname
import subprocess
import sys

from git import Repo
import yaml

from b2test_utils import working_directory
from basf2 import find_file, B2ERROR, B2INFO
from skim.registry import Registry
from skimExpertFunctions import get_test_file, get_eventN


def getAllSamples(mcCampaign):
    """Get lists of all MC and data samples to potentially test on.

    Args:
        mcCampaign (str): A label like ``MC12`` for the MC campaign to test on.

    Returns:
        mcSampleLabels (list): A list of internal MC sample labels (as used by
            `skimExpertFunctions.get_test_file`).
        dataSampleLabels (list): A list of internal data sample labels (as used by
            `skimExpertFunctions.get_test_file`).
    """
    mcSamples = [
        f"{mcCampaign}_mixedBGx1",
        f"{mcCampaign}_chargedBGx1",
        f"{mcCampaign}_ccbarBGx1",
        f"{mcCampaign}_uubarBGx1",
        f"{mcCampaign}_ddbarBGx1",
        f"{mcCampaign}_ssbarBGx1",
        f"{mcCampaign}_taupairBGx1",
        f"{mcCampaign}_mixedBGx0",
        f"{mcCampaign}_chargedBGx0",
        f"{mcCampaign}_ccbarBGx0",
        f"{mcCampaign}_uubarBGx0",
        f"{mcCampaign}_ddbarBGx0",
        f"{mcCampaign}_ssbarBGx0",
        f"{mcCampaign}_taupairBGx0",
    ]

    # For MC13 the low mult test samples exist (but they don't for MC12
    # and older) so here is a hack.
    # TODO: remove once when we regularly add the same low mult samples
    mcCampaignNumber = int(re.search(r"\d+", mcCampaign).group())
    if mcCampaignNumber > 12:
        mcSamples += [
            f"{mcCampaign}_ggBGx1",
            # f"{mcCampaign}_eeBGx1",
            f"{mcCampaign}_mumuBGx1",
            f"{mcCampaign}_eeeeBGx1",
            f"{mcCampaign}_eemumuBGx1",
            # f"{mcCampaign}_eeBGx0",
            f"{mcCampaign}_mumuBGx0",
            f"{mcCampaign}_eeeeBGx0",
            f"{mcCampaign}_eemumuBGx0",
        ]

    dataSamples = [
        "proc9_exp3",
        "proc9_exp7",
        "proc9_exp8",
        "proc10_exp7",
        "proc10_exp8",
        "proc11_exp7",
        "proc11_exp8",
        "proc11_exp10",
        "bucket9_exp12",
        "bucket10_exp12",
        "bucket11_exp12",
        "bucket12_exp12_4S_offres",
    ]

    return mcSamples, dataSamples


class CustomHelpFormatter(argparse.HelpFormatter):
    """Custom formatter for argparse which prints the valid choices for an
    argument in the help string.
    """

    def _get_help_string(self, action):
        if action.choices:
            return (
                action.help + " Valid options are: " + ", ".join(action.choices) + "."
            )
        else:
            return action.help


def required_length(*, min=None, max=None):
    """Custom action for argparse to enforce a minimum number of arguments to an option."""

    class RequiredLength(argparse.Action):
        def __call__(self, parser, args, values, option_string=None):
            if min is not None and max is None:
                if len(values) < min:
                    msg = f"Argument '{self.dest}' requires at least {min} arguments."
                    raise argparse.ArgumentTypeError(msg)
            elif min is None and max is not None:
                if len(values) > max:
                    msg = f"Argument '{self.dest}' requires at most {max} arguments."
                    raise argparse.ArgumentTypeError(msg)
            elif min is not None and max is not None:
                if len(values) < min or len(values) > max:
                    msg = f"Argument '{self.dest}' requires between {min} and {max} arguments."
                    raise argparse.ArgumentTypeError(msg)
            setattr(args, self.dest, values)

    return RequiredLength


def getArgumentParser():
    """Construct the argument parser.

    Returns:
        parser (argparse.ArgumentParser): An argument parser which obtains its
            list of valid skim names from `skim.registry`.
    """
    parser = argparse.ArgumentParser(
        description=(
            "Submits test jobs for a given set of skims, and saves the output in a "
            "format to be read by ``b2skim-stats-print``. One or more standalone or "
            "combined skim names must be provided."
        ),
        formatter_class=CustomHelpFormatter,
    )

    SkimSelector = parser.add_mutually_exclusive_group(required=True)
    SkimSelector.add_argument(
        "-s",
        "--single",
        nargs="+",
        default=[],
        choices=["all"] + Registry.names,
        metavar="skim",
        help="List of individual skims to run.",
    )
    SkimSelector.add_argument(
        "-c",
        "--combined",
        nargs="+",
        action=required_length(min=2),
        metavar=("YAMLFile", "CombinedSkim"),
        help=(
            "List of combined skims to run. This flag expects as its first argument "
            "the path to a YAML defining the combined skims. All remaining arguments "
            "are the combined skims to test. The YAML file is simply a mapping of "
            "combined skim names to the invidivual skims comprising them. For example, "
            "``feiSL: [feiSLB0, feiSLBplus]``."
        ),
    )

    parser.add_argument(
        "-n",
        type=int,
        default=10000,
        metavar="nEventsPerSample",
        dest="nEventsPerSample",
        help=(
            "Number of events to run per sample. This input can be any positive "
            "number, but the actual number events run is limited to the size of the "
            "test files (~200,000 for MC files and ~20,000 for data files)."
        ),
    )
    parser.add_argument(
        "--mccampaign",
        default="MC13",
        choices=["MC12", "MC13"],
        help="The MC campaign to test on.",
    )
    parser.add_argument(
        "--dry-run",
        "--dry",
        action="store_true",
        help="Print the submission commands, but don't run them.",
    )

    sampleGroup = parser.add_mutually_exclusive_group()
    sampleGroup.add_argument(
        "--mconly", action="store_true", help="Test on only MC samples."
    )
    sampleGroup.add_argument(
        "--dataonly", action="store_true", help="Test on only data samples."
    )

    return parser


def get_tag(repo):
    """Get the current tag name by comparing commit hashes. If not at a tag, return None."""
    def try_tag_commit(tag):
        # try-except here to avoid weird issues that come up with some of the build-release tags
        try:
            return tag.commit
        except UnicodeDecodeError:
            pass
    HEAD = repo.head.commit
    tags = repo.tags
    tag = next((tag for tag in tags if try_tag_commit(tag) == HEAD), None)
    return tag


def write_stats_metadata(logDirectory, CombinedSkimDefinitions={}):
    """Write a short file containing details about when and how this test was run."""
    UserInfo = {
        "user": getuser(),
        "site": gethostname(),
        "datetime_UTC": str(dt.utcnow()),
    }

    ReleaseDirectory = getenv("BELLE2_RELEASE_DIR")
    LocalDirectory = getenv("BELLE2_LOCAL_DIR")

    GitInfo = {}
    if ReleaseDirectory:
        GitInfo["RunWith"] = "central release"
        GitInfo["release"] = Path(ReleaseDirectory).stem

    elif LocalDirectory:
        GitInfo["RunWith"] = "local checkout"

        # Get the status of the git repo
        repo = Repo(LocalDirectory)
        tag = get_tag(repo)
        if repo.head.is_detached:
            GitInfo["HeadDetached"] = True
            if tag:
                GitInfo["tag"] = str(tag)
        else:
            GitInfo["branch"] = str(repo.active_branch)

        GitInfo["commit"] = str(repo.head.commit)
        GitInfo["UncommitedLocalChanges"] = repo.is_dirty()

    # Put it all together and write to file
    metadata = {"WhoAndWhen": UserInfo, "GitInfo": GitInfo}
    if CombinedSkimDefinitions:
        metadata["CombinedSkims"] = CombinedSkimDefinitions

    with open(logDirectory / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)


def getSamplesToRun(mcSamples, dataSamples, mcOnly=False, dataOnly=False):
    """Get a list of samples to be tested, filtered by whether the ``mcOnly``
    or ``dataOnly`` flags are provided.

    Args:
        mcSamples (list): A list of internal labels (as used by
            `skimExpertFunctions.get_test_file`) for MC samples to potentially
            test on.
        dataSamples (list): A list of internal labels (as used by
            `skimExpertFunctions.get_test_file`) for data samples to potentially
            test on.
        mcOnly (bool): Test only on MC samples.
        dataOnly (bool): Test only on data samples.

    Returns:
        samples (list): A list of internal labels for samples to be tested on.
    """
    if mcOnly:
        return mcSamples
    elif dataOnly:
        return dataSamples
    else:
        return mcSamples + dataSamples


def verify_combined_skims(CombinedSkimDefinitions, SkimsToRun):
    """Perform basic checks on combined skim definitions to catch issues early."""
    # Check all the requested combined skims are defined in the YAML file
    unrecognised = set(SkimsToRun).difference(CombinedSkimDefinitions)
    if unrecognised:
        raise ValueError(
            f"Unrecognised combined skim name{'s'*(len(unrecognised)>1)}: {', '.join(unrecognised)}."
        )

    for CombinedSkim, skims in CombinedSkimDefinitions.items():
        # Check input YAML file only contains skims listed in registry
        for skim in skims:
            if skim not in Registry.names:
                raise ValueError(
                    f"Combined skim {CombinedSkim} contains unrecognised skim: {skim}."
                )

        # Check input YAML file does not list a skim twice in one combined skim
        duplicates = set([s for s in skims if skims.count(s) > 1])
        if duplicates:
            raise ValueError(
                f"Duplicates in combined skim {CombinedSkim}: {', '.join(duplicates)}."
            )


def submit_jobs(skims, samples, nEventsPerSample, DryRun, *, CombinedSkimDefinitions={}):
    """Submit ``bsub`` jobs for each skim and for each test sample.

    Warns if any of the ``bsub`` submissions returned a non-zero exit code.
    Otherwise, prints a message summarising the job submission.

    Args:
        skims (list): A list of skim names to be run.
        samples (list): A list of internal labels for samples to be tested.
            These are read by `skimExpertFunctions.get_test_file`.
        nEventsPerSample (int): The number of events per file to run on.
        DryRun (bool): Print the submission commands, but don't run anything.
        CombinedSkimDefinitions (dict(list)): A data structure specifyign which
            individual skims comprise the combined skims. If this argument is provided,
            then this function will assume we are running combined skims.
    """
    if CombinedSkimDefinitions:
        logDirectory = Path("log", "combined").resolve()
    else:
        logDirectory = Path("log", "single").resolve()

    logDirectory.mkdir(parents=True, exist_ok=True)

    write_stats_metadata(logDirectory, CombinedSkimDefinitions)

    # Set up a cache for the metadata query, to reduce number of calls
    get_cached_eventN = lru_cache()(get_eventN)

    runner = find_file(str(Path("skim", "tools", "b2skim-run")))

    for skim in skims:
        jobIDs = []
        returnCodes = []

        for sample in samples:
            sampleFile = get_test_file(sample)

            if not Path(sampleFile).exists():
                B2ERROR(
                    f"Could not find test file {sampleFile}. `b2skim-stats-submit` is meant to be run on KEKCC."
                )
                sys.exit(1)

            logFile = Path(logDirectory, f"{skim}_{sample}.out")
            errFile = Path(logDirectory, f"{skim}_{sample}.err")
            jsonFile = Path(logDirectory, f"{skim}_{sample}.json")

            workingDirectory = Path(logDirectory, f"{skim}_{sample}")
            workingDirectory.mkdir(exist_ok=True)

            # Check that the number of events asked for doesn't exceed the number of events in the test file.
            # Removing this may confuse the stats printer.
            nTestEvents = min(nEventsPerSample, get_cached_eventN(sampleFile))

            if CombinedSkimDefinitions:
                arguments = [
                    "bsub", "-q", "l", "-oo", logFile, "-e", errFile, "-J",
                    f"{skim} {sample}", "basf2", runner, "--job-information", jsonFile, "-n",
                    str(nTestEvents), "-i", sampleFile, "--", "combined",
                    *CombinedSkimDefinitions[skim]
                ]
            else:
                arguments = [
                    "bsub", "-q", "l", "-oo", logFile, "-e", errFile, "-J",
                    f"{skim} {sample}", "basf2", runner, "--job-information", jsonFile, "-n",
                    str(nTestEvents), "-i", sampleFile, "--", "single", skim
                ]

            if DryRun:
                print(" ".join([quote(str(arg)) for arg in arguments]) + "\n")
                continue

            with working_directory(workingDirectory):
                process = subprocess.run(
                    arguments,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                )

                jobIDs.append(re.findall("\d+", str(process.stdout))[0])
                returnCodes.append(process.returncode)

        if any(returnCodes):
            B2ERROR(
                f"An error occurred while submitting jobs for {skim} skim."
            )
        elif jobIDs:
            B2INFO(
                f"Running {skim} skim on {nEventsPerSample} events from test samples of {', '.join(samples)}. Job IDs:\n  "
                + "\n  ".join(jobIDs)
            )

    B2INFO(
        f"Log files will be written to {str(logDirectory)}. "
        "Once these jobs have finished, please run `b2skim-stats-print` "
        f"from the directory {str(Path.cwd().resolve())}."
    )


if __name__ == "__main__":
    parser = getArgumentParser()
    args = parser.parse_args()

    mcSamples, dataSamples = getAllSamples(args.mccampaign)
    samples = getSamplesToRun(mcSamples, dataSamples, args.mconly, args.dataonly)

    if args.single:
        skims = args.single
        if skims == ["all"]:
            skims = Registry.names
        elif "all" in skims:
            raise ValueError("Cannot pass both 'all' and a list of skim names.")

        submit_jobs(skims, samples, args.nEventsPerSample, args.dry_run)
    else:
        with open(args.combined[0]) as YAMLFile:
            CombinedSkimDefinitions = yaml.safe_load(YAMLFile)
        skims = args.combined[1:]
        if skims == ["all"]:
            skims = CombinedSkimDefinitions.keys()
        elif "all" in skims:
            raise ValueError("Cannot pass both 'all' and a list of skim names.")

        verify_combined_skims(CombinedSkimDefinitions, skims)
        submit_jobs(skims, samples, args.nEventsPerSample, args.dry_run, CombinedSkimDefinitions=CombinedSkimDefinitions)
